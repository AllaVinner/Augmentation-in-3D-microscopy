{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parser for PlantSeg\n",
    "Parse the data in PNAS to fit with PlantSeg input specifications.\n",
    "\n",
    "The data will be copied into the folder 'PNAS-PlantSeg'. There, each 3D volume will be stored in a file 'sample_<i>.h5'\n",
    "This file contains bothe the raw data stored in 'raw' and the labels in 'label'. \n",
    "I should perhaps split them into training and evaluation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "from os.path import join\n",
    "source_path = '/scratch/ottosson/datasets/SAM/data/PNAS'\n",
    "destination_path = '/scratch/ottosson/datasets/SAM/data/PNAS-PlantSeg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all directories in source path\n",
    "plants = [dir for dir in os.listdir(source_path) if os.path.isdir(os.path.join(source_path, dir))] \n",
    "# Set intermediat paths \n",
    "intermediate_input_path = 'processed_tiffs'\n",
    "intermediate_target_path = 'segmentation_tiffs'\n",
    "# Set strings which are unique for input and output to be able to tell them apart\n",
    "determiner_input_string = 'acylYFP'\n",
    "determiner_output_string = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_label_file(file, directory):\n",
    "    \"\"\"\n",
    "    Finds the files in the directory which share the timestamp and plant name of 'file'\n",
    "    The file is assumed to be named \"time_plant_XXXXXX.XX\"\n",
    "    \"\"\"\n",
    "    parts = file.split(\"_\")\n",
    "    time = parts[0]\n",
    "    plant = parts[1]\n",
    "    matched_files = []\n",
    "    for f in os.listdir(directory):\n",
    "        f_parts = f.split(\"_\")\n",
    "        f_time = f_parts[0]\n",
    "        f_plant = f_parts[1]\n",
    "        if time == f_time and plant == f_plant:\n",
    "            matched_files.append(f)\n",
    "    if len(matched_files) != 1:\n",
    "        print(f\"Wrong number of file matched: {matched_files}\\nfile: {file}\\nDir: {directory}\\n\")\n",
    "    if len(matched_files) == 0:\n",
    "        return None\n",
    "    return matched_files[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrong number of file matched: []\n",
      "file: 40hrs_plant18_trim-acylYFP.tif\n",
      "Dir: /scratch/ottosson/datasets/SAM/data/PNAS/plant18/segmentation_tiffs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_i = 0\n",
    "# Go trhough all plant 'movies'\n",
    "for plant in plants:\n",
    "    # Get all frames in 'movie'\n",
    "    plant_path = join(source_path,plant)\n",
    "    files = os.listdir(join(plant_path,intermediate_input_path))\n",
    "    # Go through all frames\n",
    "    for file in files:\n",
    "        # Skip files which are not training\n",
    "        if determiner_input_string not in file: continue \n",
    "        # Find target file corresponding to input file\n",
    "        input_path = join(plant_path,intermediate_input_path,file)\n",
    "        target_name = find_label_file(file,join(plant_path,intermediate_target_path))\n",
    "        # If no target file, contnue\n",
    "        if not target_name: continue\n",
    "        \n",
    "        # Create paths to targets and inputs\n",
    "        target_path = join(plant_path,intermediate_target_path,target_name)\n",
    "        sample_path = join(destination_path,f\"sample_{sample_i}.h5\")\n",
    "        \n",
    "        # Create a new folder for the restructured data.\n",
    "        raw = imageio.volread(input_path)\n",
    "        label = imageio.volread(target_path)\n",
    "        with h5py.File(sample_path, 'w') as hf:\n",
    "            hf.create_dataset('raw', data=raw)\n",
    "            hf.create_dataset('label', data=label)\n",
    "        sample_i = sample_i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ccebf19bffab00ad3f71fce418a4feacbf84866d37317da41dc1c9aeaa67697d"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit ('plant-seg': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
